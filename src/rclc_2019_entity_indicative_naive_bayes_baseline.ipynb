{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einb\n",
    "from data_utils import load_rclc_corpus, read_pub_text \n",
    "from eval_utils import top5UptoD_err, top5UptoD_prec\n",
    "from sentence_filtering import SentenceFilterClass\n",
    "from statistics import mean \n",
    "from urllib.parse import urlparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RCLC Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading datasets: 100%|##########| 22/22 [00:09<00:00,  3.21it/s]\n",
      "loading pubs: 100%|##########| 179/179 [00:00<00:00, 1996.20it/s]\n"
     ]
    }
   ],
   "source": [
    "TEXT_PATH = '../data/resource/pubs/text/'\n",
    "corpus = load_rclc_corpus('../data/corpus.jsonld', \n",
    "                          '../data/resource/dataset/html/',\n",
    "                          '../data/resource/pubs/json/')\n",
    "datasets = corpus['datasets']\n",
    "pubs = corpus['pubs']\n",
    "sent_filter = SentenceFilterClass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prepare the publications for our experiment. \n",
    "\n",
    "We extract abstract, content, and references from the publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of publications: 179\n"
     ]
    }
   ],
   "source": [
    "pub_contexts = []\n",
    "pub_labels = []\n",
    "for pub in pubs:\n",
    "    context = [pub['dct:title']['@value'], pub['dct:publisher']['@value']]\n",
    "    parsed = pub['parsed_pub']\n",
    "    metadata = parsed['metadata']\n",
    "    if metadata['abstractText']:\n",
    "        context.append(metadata['abstractText'])\n",
    "\n",
    "    if metadata['sections']:\n",
    "        s_text = [sec['text'] for sec in metadata['sections'] if len(sec['text']) > 0]\n",
    "    else:\n",
    "        _id = urlparse(pub['@id']).fragment.split('-')[1]\n",
    "        s_text = read_pub_text(TEXT_PATH + _id + '.pdf.txt')\n",
    "    context.extend(s_text)\n",
    "    \n",
    "    if metadata['references']:\n",
    "        ref_titles = [ref['title'] for ref in metadata['references']]\n",
    "    context.extend(ref_titles)\n",
    "    \n",
    "    filtered_sent = sent_filter.final_approach(' '.join(context))\n",
    "    pub_contexts.append(' '.join(filtered_sent))\n",
    "    \n",
    "    \n",
    "    citations = pub['cito:citesAsDataSource']\n",
    "    if isinstance(citations, list):\n",
    "        pub_label = [c['@id'] for c in citations]\n",
    "    elif isinstance(citations, dict):\n",
    "        pub_label = [citations['@id']]\n",
    "    pub_labels.append(pub_label)\n",
    "    \n",
    "print(f'Number of publications: {len(pub_contexts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, we want to see whether our model can overfit the corpus\n",
    "\n",
    "We train the baseline model on all publications in the corpus, then we predict the same set of publications. The baseline model is based on our [rcc submission](https://github.com/LARC-CMU-SMU/coleridge-rich-context-larc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top5UptoD error rate: 0.09925512104283053\n",
      "top5UptoD_precision: 0.9007448789571695\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "dataset_word_dict, word_dataset_dict = einb.parameter_learn(pub_contexts,pub_labels)\n",
    "\n",
    "# Perform prediction\n",
    "errs = []\n",
    "precs = []\n",
    "for i, context in enumerate(pub_contexts):\n",
    "    # print(f'{pub_labels[i]}')\n",
    "    preds = einb.predict(context, dataset_word_dict, word_dataset_dict, 5)\n",
    "    # print(f'{preds}')\n",
    "    errs.append(top5UptoD_err(pub_labels[i], [p[0] for p in preds]))\n",
    "    precs.append(top5UptoD_prec(pub_labels[i], [p[0] for p in preds]))\n",
    "print(f'top5UptoD error rate: {mean(errs)}')\n",
    "print(f'top5UptoD_precision: {mean(precs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, We conduct 5-fold Cross Validation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "top5UptoD error rate: 0.3972222222222222\n",
      "top5UptoD_precision: 0.6027777777777777\n",
      "Fold 1\n",
      "top5UptoD error rate: 0.3365740740740741\n",
      "top5UptoD_precision: 0.663425925925926\n",
      "Fold 2\n",
      "top5UptoD error rate: 0.4425925925925926\n",
      "top5UptoD_precision: 0.5574074074074075\n",
      "Fold 3\n",
      "top5UptoD error rate: 0.31574074074074077\n",
      "top5UptoD_precision: 0.6842592592592592\n",
      "Fold 4\n",
      "top5UptoD error rate: 0.3880952380952381\n",
      "top5UptoD_precision: 0.611904761904762\n",
      "Average top5UptoD error rate: 0.37604497354497357\n",
      "Average top5UptoD_precision: 0.6239550264550264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "cv_errs = []\n",
    "cv_precs = []\n",
    "iter = 0\n",
    "for train_index, test_index in kf.split(pub_contexts):\n",
    "    print(f'Fold {iter}')\n",
    "    X_train = [pub_contexts[i] for i in train_index]\n",
    "    X_test = [pub_contexts[i] for i in test_index]\n",
    "    y_train = [pub_labels[i] for i in train_index]\n",
    "    y_test = [pub_labels[i] for i in test_index]\n",
    "    \n",
    "    dataset_word_dict, word_dataset_dict = einb.parameter_learn(X_train,y_train)\n",
    "    errs = []\n",
    "    precs = []\n",
    "    for i, context in enumerate(X_test):\n",
    "        # print(f'{y_test[i]}')\n",
    "        preds = einb.predict(context, dataset_word_dict, word_dataset_dict, 5)\n",
    "        # print(f'{preds}')\n",
    "        errs.append(top5UptoD_err(y_test[i], [p[0] for p in preds]))\n",
    "        precs.append(top5UptoD_prec(y_test[i], [p[0] for p in preds]))\n",
    "    cv_errs.append(mean(errs))\n",
    "    cv_precs.append(mean(precs))\n",
    "    print(f'top5UptoD error rate: {mean(errs)}')\n",
    "    print(f'top5UptoD_precision: {mean(precs)}')\n",
    "    iter += 1\n",
    "\n",
    "print(f'Average top5UptoD error rate: {mean(cv_errs)}')\n",
    "print(f'Average top5UptoD_precision: {mean(cv_precs)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
